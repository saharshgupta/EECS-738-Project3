{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_neuron_to_another_PokerHands.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lvxWPmRlQsVW",
        "0t1I2ecTQj34"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvxWPmRlQsVW"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "Importing pandas for data handling, and numpy for math <br>\n",
        "Using Plot style fivethirtyeight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h73neoXmsYQx"
      },
      "source": [
        "# Importing libraries required to handle the data and matrix calculations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importing libraries required for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing libraries required for importing the data\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# For time measurements\n",
        "import time\n",
        "\n",
        "# Using plot style from fivethirtyeight\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1I2ecTQj34"
      },
      "source": [
        "## Neuron and Layers\n",
        "\n",
        "* **Neuron Class:** The neuron class is just a representation of a neuron with a bias and some incoming weights\n",
        "\n",
        "* **Neuron Layer Class:** The layer class initializes a fully connected layer (Dense layer)based on the inputs and the number of neurons in a layer, uses the Neuron base class for neuron representation\n",
        "\n",
        "The weight initialization is from the standard normal distribution ~ N(0,1).\n",
        "\n",
        "The Bias is initialized with 0.\n",
        "\n",
        "The Neuron Layer has the following properties:\n",
        "* The Weights matrix shape is : [inputs x number of neurons in this layer]\n",
        "* The biases in a layer, we have one bias per neuron and so the shape is: \n",
        "[1 x number of neurons in this layer]\n",
        "* The fire function: Takes in the inputs and fires the layer, i.e. calculates the weighted sum of incoming weights and the input, adds the bias to get the output for the layer. Also calculates the activation,a dn the gradient of the activation\n",
        "* ReLU (Rectified Linear Unit) activation function: f(x) = {0 if x<= 0; and x if x > 0}\n",
        "* ReLU gradient: Assuming the derivative for RelU at 0 is 0,a s it approaches 0\n",
        "* Other helper functions for getting and setting weights and biases\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgTW38IHuYpl"
      },
      "source": [
        "class Neuron:\n",
        "  def __init__(self, w = None, b = None):\n",
        "    self.weights = w\n",
        "    self.bias = b\n",
        "\n",
        "  def update(self, w, b):\n",
        "    self.weights = w\n",
        "    self.bias = b\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"{{My Bias = {self.bias} | Incoming Weights: {self.weights}}}\"\n",
        "\n",
        "class NeuronLayer:\n",
        "  # Every layer has an input vector (x: previous layer output shape (nodes)) and an output vector (y: This layer output shape (nodes/workers))\n",
        "  def __init__(self,n_inputs,n_neurons, weights = [], biases = []):\n",
        "    self.input_nodes = n_inputs # Nodes in the previous layer (as they are input to this layer)\n",
        "    self.output_nodes = n_neurons # Nodes in the layer\n",
        "    \n",
        "    if len(weights) == 0: # If no weights were provided\n",
        "      self.weights = np.random.rand(self.input_nodes,self.output_nodes)\n",
        "    else:\n",
        "      self.weights = weights\n",
        "\n",
        "    if len(biases) == 0: # If no biases were provided\n",
        "      # self.biases = np.random.randn(1,self.output_nodes)\n",
        "      self.biases = np.zeros((1,self.output_nodes))\n",
        "    else:\n",
        "      self.biases = biases\n",
        "\n",
        "    # Creating the specific layer with the weights and biases for the Neurons\n",
        "    self.layer = [Neuron(w,b) for b,w in zip(self.biases[0], self.weights.T)]\n",
        "\n",
        "  def fire(self,training_input,activationFunction, gradientFunction):\n",
        "    self.output = np.dot(training_input, self.weights) + self.biases\n",
        "    self.activated_output = np.array([[activationFunction(c) for c in r] for r in self.output])\n",
        "    self.activation_gradient = np.array([[gradientFunction(c) for c in r] for r in self.output])\n",
        "\n",
        "  # Using Rectified Linear Unit activation function, if the value is negative return 0 otherwise unchanged value\n",
        "  # f(x) = {0 if x<= 0; and x if x > 0}\n",
        "  # Implementation is just a max{0,value}\n",
        "  def relu(self,x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  # The derivative for f(x) = {0 if x <= 0; and x if x > 0}\n",
        "  # f'(x) = { 0 if x <= 0; and 1 if x > 0} assuming the derivative for relu at 0 is 0 (as its approaching 0)\n",
        "  def reluGradient(self,x):\n",
        "    if x <= 0:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "  # Helper functions to adjust/update the weights and biases\n",
        "  def getWeights(self):\n",
        "    return self.weights\n",
        "  def getBiases(self):\n",
        "    return self.biases\n",
        "  def setWeights(self,new_weights):\n",
        "    self.weights = new_weights\n",
        "  def setBiases(self,new_biases):\n",
        "    self.biases = new_biases\n",
        "\n",
        "  def updateLayer(self):\n",
        "    for i,b,w in zip(range(self.output_nodes), self.biases[0], self.weights.T):\n",
        "      self.layer[i].update(w,b) \n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"\\n{self.layer}\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEO7IftDQgPG"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "The Neural Network class uses the Neuron layer class defined above to create a network of layers.\n",
        "\n",
        "The input layer is created for representation purposes as it does nothing in the calculations as it has no incoming weights or biases.\n",
        "\n",
        "The initialization of the Neural Network requires a list of neurons per layer, with the list provided the input, hidden and output layers are created.\n",
        "\n",
        "The training of a neural network has 2 steps:\n",
        "* Feed forward: In this step we just feed the data forward, starting from the provided input, the first hidden layer is given that input, the layer is fired where the activations are calculated, these output activation of this layer is used as the input for the next layer, and hence feed forward.\n",
        "\n",
        "* Backpropagation: This is the main step where the error is calculated from the given labels and predicted output. With this error we calculate the gradient descent for the error (loss) function. \n",
        "\n",
        "The gradient descent for the output layer = \n",
        "error_derivative * activation_derivative\n",
        "\n",
        "For weights: the Input to this layer is multiplied to get the weighted error based on the inputs, that is which input mattered more in this calculations and so which weights need to change more than others. It is then multiplied with learning rate to control the changes.\n",
        "\n",
        "For biases: The gradient descent is summed up along the axis and then we have the biases per node based on the error and the learning rate.\n",
        "\n",
        "Loss Functions:\n",
        "Used basic squared error as it gives positive results, also tried mean squarred error and root mean squarred error.\n",
        "The functions and their derivaties are used to calculate the gradient descent\n",
        "\n",
        "The Fit Function: Takes input the whole training data, number of epochs, learning rate, error threshold and mini batch size.\n",
        "\n",
        "Given these parameters, the fit function runs a loop for all the epochs.\n",
        "\n",
        "    For every epoch:\n",
        "      The training data is shuffled,\n",
        "      The data is split into mini batches of the provided size,\n",
        "      for every mini batch:\n",
        "        We feed forward the mini batch and the error is calculated\n",
        "        The error is backpropagated\n",
        "    The netowrk weights and biases are updated once the training is complete.\n",
        "    The fit method returns an array with errors for every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHVvxs11s5gp"
      },
      "source": [
        "class NN:\n",
        "  # layer_sizes format is a list of number of neurons per layer for example => [2,3,1] 2 input neurons, 3 hidden neurons and 1 output neuron\n",
        "  def __init__(self,layer_sizes):\n",
        "    # Network parameters initialization\n",
        "    self.network_size = len(layer_sizes)\n",
        "    self.input_nodes = layer_sizes[0]\n",
        "    self.output_nodes = layer_sizes[self.network_size-1]\n",
        "    self.hidden_layers = [None for _ in range(self.network_size-2)]\n",
        "    self.layers = []\n",
        "\n",
        "    # Initializing Network architecture/layers\n",
        "    # For the provided structure we create the nodes/workers\n",
        "    # The input layer is easy as it only has nodes for every input with no incomming weights and biases of its own\n",
        "    # Creating this layer for representation purposes, doesnt have any work in the neural networks calculations\n",
        "    self.input_layer = [Neuron() for _ in range(self.input_nodes)]\n",
        "    \n",
        "    # Creating Hidden layers\n",
        "    for i in range(self.network_size -2): # Starting from 0 as input to first hidden layer is from the input layer\n",
        "      self.hidden_layers[i] = NeuronLayer(layer_sizes[i],layer_sizes[i+1])\n",
        "      self.layers.append(self.hidden_layers[i])\n",
        "\n",
        "    # Creating Output Layer\n",
        "    self.output_layer = NeuronLayer(layer_sizes[self.network_size-2],self.output_nodes)\n",
        "    self.layers.append(self.output_layer)\n",
        "\n",
        "  def epochRange(self, start, epochs):\n",
        "    epoch = start\n",
        "    while epoch < epochs:\n",
        "      yield (epoch)\n",
        "      epoch += 1\n",
        "\n",
        "  def squaredError(self, predicted, true):\n",
        "    error = 0.5*(true - predicted)**2\n",
        "    return np.sum(error)\n",
        "\n",
        "  def squaredGradient(self, predicted, true):\n",
        "    return (true - predicted)\n",
        "\n",
        "  # A mean squared cost function, takes the mean of the error term that is the squared diffrence between the predicted and expected value\n",
        "  # mse = sum((x_true - x_predicted)^2)/2N = mean(0.5*(x_true - x_predicted)^2)\n",
        "  def meanSquaredError(self,predicted,true):\n",
        "    # Calculating the error\n",
        "    error = true - predicted # [[predicted_i1, predicted_i2, ..., predicted_in]...] -[[true_i1, true_i2, ..., true_in]...] shape: rows=> number of samples in batch (b); col=> input features (n); => [b x n] matrix\n",
        "    error = 0.5 * np.power(error, 2) # Squarred error [[error_sample1]**2,[error_sample2]**2, ..., [error_samplen]**2] => [b x n] matrix\n",
        "    return np.mean(error) # Mean of the squared error within a batch, does nothing if batch size is 1 that is 1 sample at a time\n",
        "\n",
        "  # The derivative of mse = sum((x_true - x_predicted)^2)/2N\n",
        "  # mse' = (x_true - x_predicted)/N\n",
        "  # Similar implementation as the mean squared error\n",
        "  def mseGradient(self,predicted,true):\n",
        "    error = true - predicted # [[predicted_i1, predicted_i2, ..., predicted_in]...] -[[true_i1, true_i2, ..., true_in]...] shape: number of samples in batch (b) x input features (n); => [b x n] matrix\n",
        "    return error/true.size # Mean of the error within a batch, does nothing if batch size is 1 that is 1 sample at a time\n",
        "\n",
        "  # Another cost function, takes the root of the mean squarred error\n",
        "  def rootMeanSquaredError(self,predicted,true):\n",
        "    # Calculating the error\n",
        "    error = true - predicted # [[predicted_i1, predicted_i2, ..., predicted_in]...] -[[true_i1, true_i2, ..., true_in]...] shape: rows=> number of samples in batch (b); col=> input features (n); => [b x n] matrix\n",
        "    error = np.power(error, 2) # Squarred error [[error_sample1]**2,[error_sample2]**2, ..., [error_samplen]**2] => [b x n] matrix\n",
        "    return np.sqrt(np.mean(error)) # the square root of the mean squared error\n",
        "\n",
        "  # The derivative for the rmse function\n",
        "  def rmseGradient(self,predicted,true):\n",
        "    return self.mseGradient(predicted,true)/(2*self.rootMeanSquarredError(predicted,true)) # d(rmse)/d(prev_activation) = (1/2*rmse)*mseGradient()\n",
        "    \n",
        "  def gradientDescent(self,error_gradient, activation_gradient):\n",
        "    return error_gradient*activation_gradient\n",
        "\n",
        "  def feedForward(self, training_batch):\n",
        "    inp = training_batch # Input Layer\n",
        "\n",
        "    for i in self.epochRange(0,self.network_size-2): # For every hidden layer (Total network layers - input layer - output layer)\n",
        "      self.hidden_layers[i].fire(inp, self.hidden_layers[i].relu, self.hidden_layers[i].reluGradient)\n",
        "      inp = self.hidden_layers[i].activated_output # Feeding the output forward as input\n",
        "    \n",
        "    # output layer (input is the output from the last layer (input/hidden layer))\n",
        "    self.output_layer.fire(inp, self.output_layer.relu, self.output_layer.reluGradient)\n",
        "    # print(f\"Final output: {self.output_layer.activated_output}\")\n",
        "\n",
        "  # Going in reverse from the output layer to the first hidden layer in the network\n",
        "  def backPropagation(self, training_batch, labels, lr, threshold, prev_error):\n",
        "    # Calculating the error and the error gradient (the direction of the gradient ascent)\n",
        "    error = self.squaredError(self.output_layer.activated_output,labels)\n",
        "    error_gradient = self.squaredGradient(self.output_layer.activated_output,labels)\n",
        "    # error = self.meanSquaredError(self.output_layer.activated_output,labels)\n",
        "    # error_gradient = self.mseGradient(self.output_layer.activated_output,labels)\n",
        "\n",
        "    # Calculating the asjustments for the output layer based on the error and the output layer activation gradient\n",
        "    adjustments = self.gradientDescent(error_gradient, self.output_layer.activation_gradient)\n",
        "    \n",
        "    if self.network_size > 2: # If there is a hidden layer\n",
        "      last_input = self.hidden_layers[-1].activated_output # The activated output of the last hidden layer\n",
        "    else:\n",
        "      last_input = training_batch # No hidden layer, the output layer is the only layer in the network\n",
        "\n",
        "    # Calculating output layer adjustments based on the loss\n",
        "    adjustments_wo = np.dot(last_input.T, adjustments) * lr\n",
        "    output_bias = np.sum(adjustments,axis=0,keepdims=True) * lr\n",
        "\n",
        "    adjustments_wh = []\n",
        "    hidden_bias = []\n",
        "    runCount = 0\n",
        "\n",
        "    # For hidden layers, the adjustments from the previous layer is used and multiplied with the weights to get the gradient of cost with respect to the previous activation\n",
        "    for i in range(self.network_size-3, -1, -1): # For every hidden layer (Total network layers - input layer - output layer)\n",
        "      if runCount != 0:\n",
        "        # Following hidden layers towards input layers\n",
        "        adjustments = np.dot(adjustments, self.hidden_layers[i+1].weights.T)\n",
        "        adjustments = adjustments * self.hidden_layers[i].activation_gradient\n",
        "      else:\n",
        "        # Last Hidden layer adjustments\n",
        "        adjustments = np.dot(adjustments, self.output_layer.weights.T)\n",
        "        adjustments = adjustments * self.hidden_layers[i].activation_gradient\n",
        "        \n",
        "      if i > 0: # If there are hidden layers\n",
        "        last_input = self.hidden_layers[i-1].activated_output # The activated output of the previous hidden layer\n",
        "      else: # First hidden layer takes the provided input\n",
        "        last_input = training_batch # No hidden layer, the output layer is the only layer in the network\n",
        "\n",
        "      runCount += 1\n",
        "      adjustments_wh.append(np.dot(last_input.T, adjustments) * lr)\n",
        "      hidden_bias.append(np.sum(adjustments,axis=0,keepdims=True) * lr)\n",
        "      \n",
        "\n",
        "    # Updating the weights and biases from the calculated adjustments\n",
        "    self.output_layer.setWeights(self.output_layer.getWeights() + adjustments_wo)\n",
        "    self.output_layer.setBiases(self.output_layer.getBiases() + output_bias)\n",
        "\n",
        "    for i,wh,bh in zip(range(self.network_size-3, -1, -1),adjustments_wh,hidden_bias):\n",
        "      self.hidden_layers[i].setWeights(self.hidden_layers[i].getWeights() + wh)\n",
        "      self.hidden_layers[i].setBiases(self.hidden_layers[i].getBiases() + bh)\n",
        "\n",
        "    return error\n",
        "\n",
        "  def fit(self,X_train,labels,epochs, mini_batch_size = 32,learningRate = 1e-1, threshold = 1e-5):\n",
        "    lr = learningRate # Learning rate\n",
        "    error = np.zeros((epochs+1,1))\n",
        "    error[0] = np.inf # Starting with a very high error,a nd minimizing this using gradient descent\n",
        "    training_data = np.hstack((X_train,labels))\n",
        "    n = len(training_data)\n",
        "    for epoch in self.epochRange(1,epochs+1): # For every epoch\n",
        "      print(f\"Epoch: {epoch} => \", end = \"\")\n",
        "      start = time.time()\n",
        "\n",
        "      # Shuffling the data and creating mini batches\n",
        "      np.random.shuffle(training_data)\n",
        "      mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "      m_count = 0\n",
        "\n",
        "      for mini_batch in mini_batches: # For every mini batch\n",
        "        if m_count % mini_batch_size == 0:\n",
        "          print(\".\", end=\"\")\n",
        "        training_batch = mini_batch[:,:-self.output_layer.output_nodes] # Shuffled X_train\n",
        "        training_labels = np.array([mini_batch[:,-self.output_layer.output_nodes]]).T # Shuffled Y_train\n",
        "\n",
        "        # Run the feed forward\n",
        "        self.feedForward(training_batch)\n",
        "\n",
        "        #Calculating error and backpropogating the updates\n",
        "        error[epoch] += self.backPropagation(training_batch, training_labels, learningRate, threshold, error[epoch-1])\n",
        "        m_count += 1\n",
        "\n",
        "      end = time.time()\n",
        "      print(f\" => Error: {error[epoch,0]}; \", end = \"\")\n",
        "      print(f\"Time taken: {end - start} seconds\")\n",
        "\n",
        "    self.updateNetwork() # Updating the wholel network with the final weights and biases calculated\n",
        "    return error[1:epoch+1]\n",
        "\n",
        "  # Updates the network, sets the final values of the neuron after all backpropogation through every epoch. Only required for final representation of the network, as it updates all the neurons in every layer\n",
        "  def updateNetwork(self):\n",
        "    self.output_layer.updateLayer() # Updating the output layer neurons\n",
        "    for i in self.epochRange(0,self.network_size-2): # For every hidden layer (Total network layers - input layer - output layer)\n",
        "      self.hidden_layers[i].updateLayer() # Updating the hidden layer neurons\n",
        "\n",
        "  def classify(self, x):\n",
        "    if x >= 0.5:\n",
        "      return 1\n",
        "    return 0\n",
        "  def __repr__(self):\n",
        "    if self.network_size > 2:\n",
        "      return f\"Neural Network Architecture \\n\\nInput Layer=> \\n{self.input_layer} \\n\\nHidden Layers=> {self.hidden_layers} \\n\\nOutput Layer=> {self.output_layer} \\n\"\n",
        "    return f\"Neural Network Architecture \\n\\nInput Layer=> \\n{self.input_layer} \\n\\nOutput Layer=> {self.output_layer} \\n\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeCr2NoQVIh"
      },
      "source": [
        "##Loading and Pre-Processing the dataset\n",
        "\n",
        "Reading the 5 Suites and 5 Cards and their respective class\n",
        "\n",
        "Cleaning the dataset, Dropping rows with NA values and resetting index\n",
        "\n",
        "Scaling the values for better training, and splitting into train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "R6oFzgNG8YFR",
        "outputId": "51abc3db-fc40-48a5-a6c9-9d69360ec669"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d99d493d-e097-4a15-acda-4c51d3121a53\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d99d493d-e097-4a15-acda-4c51d3121a53\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving poker-hand-testing.data to poker-hand-testing.data\n",
            "Saving poker-hand-training-true.data to poker-hand-training-true.data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxewzCcm_qls"
      },
      "source": [
        "training_path = io.BytesIO(uploaded['poker-hand-training-true.data'])\n",
        "testing_path = io.BytesIO(uploaded['poker-hand-testing.data'])\n",
        "df_train = pd.read_csv(training_path, header = None, names = ['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'Class'])\n",
        "df_test = pd.read_csv(testing_path, header = None, names = ['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'Class'])\n",
        "df_train = df_train.sample(frac=1).reset_index(drop=True) # Shuffling the training data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLhfUzHBBjWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "31c07b22-76c1-403f-aabf-b88c501358e8"
      },
      "source": [
        "# Poker Hands dataset\n",
        "df = df_train.copy()\n",
        "\n",
        "target = 'Class'\n",
        "predictors = list(df.columns)\n",
        "predictors.remove(target)\n",
        "\n",
        "suites = ['S1', 'S2', 'S3', 'S4', 'S5']\n",
        "cards = ['C1', 'C2', 'C3', 'C4', 'C5']\n",
        "\n",
        "suitesNorm = df_train['S1'].max() # Suite values=> [1,2,3,4] \n",
        "cardsNorm = df_train['C1'].max() # Card values=> [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
        "classNorm = df_train['Class'].max() # Class values=> [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "for s in suites: # => dividing by 4 => [0.25,0.5,0.75,1]\n",
        "  df[s] = df[s]/suitesNorm\n",
        "\n",
        "for c in cards: # dividing the values by 13 => so they are between [0,1]\n",
        "  df[c] = df[c]/cardsNorm\n",
        "\n",
        "# Scaling the target values in range [0,1]\n",
        "df[target] = df[target]/classNorm\n",
        "\n",
        "X = df[predictors].values # Scaled Features\n",
        "y = np.array([df[target].values]).T # Target values\n",
        "\n",
        "train_len = round(0.8*len(X)) # 80-20 split for train and test dataset\n",
        "X_train = X[:train_len] # Features to train on\n",
        "y_train = y[:train_len] # Target values for backpropagation\n",
        "\n",
        "X_test = X[train_len:] # Features to test on\n",
        "y_test = y[train_len:] # Target values for performance testing (accuracy/error)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S1</th>\n",
              "      <th>C1</th>\n",
              "      <th>S2</th>\n",
              "      <th>C2</th>\n",
              "      <th>S3</th>\n",
              "      <th>C3</th>\n",
              "      <th>S4</th>\n",
              "      <th>C4</th>\n",
              "      <th>S5</th>\n",
              "      <th>C5</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     S1        C1    S2        C2  ...        C4    S5        C5     Class\n",
              "0  0.75  0.461538  1.00  0.153846  ...  0.461538  0.50  0.461538  0.333333\n",
              "1  0.50  0.461538  0.75  0.230769  ...  1.000000  0.25  0.538462  0.000000\n",
              "2  0.75  0.692308  0.25  0.923077  ...  0.230769  0.50  0.076923  0.000000\n",
              "3  0.75  0.307692  0.50  0.769231  ...  1.000000  1.00  0.307692  0.111111\n",
              "4  0.75  0.769231  0.25  0.461538  ...  1.000000  0.25  0.384615  0.000000\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnNFJ3xKQZ0I"
      },
      "source": [
        "## Training the Neural network\n",
        "\n",
        "For the archtecture of this network I played around with many different combinations, increasing the number of nodes/width in the hidden layer (increasing the representaion dimension of the data) helped in lowering the error, also gradual increase in the number of layers/depth also helped with the error. So the final choice for the architecure of the network is as follows:\n",
        "\n",
        "10 Neurons Input layer, 20 Neurons in the first hidden layer, 40 in the next hidden layer, then back to 20 in the next hidden layer, then back to 10 in the last hidden layer and then to a single output neuron in the output layer for the regression value of the evaluated poker hand\n",
        "\n",
        "Trained for 30 epochs with a learning rate of 1e-12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVyDdvgttrvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5f2630-4eb9-4f40-ebdc-bc416ea17b41"
      },
      "source": [
        "np.random.seed(1) # Seeding for reproducibility\n",
        "myNN = NN([X_train.shape[1], 20, 40, 20, 10, 1])\n",
        "myNNerror = myNN.fit(X_train,y_train,epochs= 30, learningRate= 1e-12)\n",
        "# myNN"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 => .................... => Error: 623946047294.6753; Time taken: 6.454521894454956 seconds\n",
            "Epoch: 2 => .................... => Error: 728096.0439337145; Time taken: 6.343914747238159 seconds\n",
            "Epoch: 3 => .................... => Error: 586.9168189812918; Time taken: 6.3261895179748535 seconds\n",
            "Epoch: 4 => .................... => Error: 274.86671545354517; Time taken: 6.351655960083008 seconds\n",
            "Epoch: 5 => .................... => Error: 209.70325626608295; Time taken: 6.290038108825684 seconds\n",
            "Epoch: 6 => .................... => Error: 182.037814498096; Time taken: 6.346559286117554 seconds\n",
            "Epoch: 7 => .................... => Error: 166.9683051187988; Time taken: 6.301187753677368 seconds\n",
            "Epoch: 8 => .................... => Error: 157.63406612556537; Time taken: 6.3210883140563965 seconds\n",
            "Epoch: 9 => .................... => Error: 151.4733040784596; Time taken: 6.407973289489746 seconds\n",
            "Epoch: 10 => .................... => Error: 147.0652327076757; Time taken: 6.318653583526611 seconds\n",
            "Epoch: 11 => .................... => Error: 143.8222899702785; Time taken: 6.301166296005249 seconds\n",
            "Epoch: 12 => .................... => Error: 141.38632050575285; Time taken: 6.362734317779541 seconds\n",
            "Epoch: 13 => .................... => Error: 139.48166952192952; Time taken: 6.429271697998047 seconds\n",
            "Epoch: 14 => .................... => Error: 137.9642544783342; Time taken: 6.382469654083252 seconds\n",
            "Epoch: 15 => .................... => Error: 136.71595539189317; Time taken: 6.370368003845215 seconds\n",
            "Epoch: 16 => .................... => Error: 135.69114304752978; Time taken: 6.325506210327148 seconds\n",
            "Epoch: 17 => .................... => Error: 134.83144788859028; Time taken: 6.334066152572632 seconds\n",
            "Epoch: 18 => .................... => Error: 134.0933161649245; Time taken: 6.403596639633179 seconds\n",
            "Epoch: 19 => .................... => Error: 133.4573797318524; Time taken: 6.409283638000488 seconds\n",
            "Epoch: 20 => .................... => Error: 132.90360014938423; Time taken: 6.367584943771362 seconds\n",
            "Epoch: 21 => .................... => Error: 132.40196763246817; Time taken: 6.311478853225708 seconds\n",
            "Epoch: 22 => .................... => Error: 131.94863178956712; Time taken: 6.36769700050354 seconds\n",
            "Epoch: 23 => .................... => Error: 131.54522012749513; Time taken: 6.3680195808410645 seconds\n",
            "Epoch: 24 => .................... => Error: 131.1880755440264; Time taken: 6.334822177886963 seconds\n",
            "Epoch: 25 => .................... => Error: 130.8700477589331; Time taken: 6.400668621063232 seconds\n",
            "Epoch: 26 => .................... => Error: 130.58567375210433; Time taken: 6.305345296859741 seconds\n",
            "Epoch: 27 => .................... => Error: 130.333591348219; Time taken: 6.342291355133057 seconds\n",
            "Epoch: 28 => .................... => Error: 130.09920969104917; Time taken: 6.368982315063477 seconds\n",
            "Epoch: 29 => .................... => Error: 129.87409489034303; Time taken: 6.381012201309204 seconds\n",
            "Epoch: 30 => .................... => Error: 129.66523081571793; Time taken: 6.317070960998535 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w8BbQT1RJ61"
      },
      "source": [
        "## Testing the trained Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKB_jZcoi22W"
      },
      "source": [
        "The error on the testing dataset is higher than the error of the last epoch on the training data, this could mean that the network could have been overfitted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPKiwzU1C9_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eccc178-c5eb-4dbb-90a0-b3089784f390"
      },
      "source": [
        "myNN.feedForward(X_test)\n",
        "test_output = myNN.output_layer.activated_output\n",
        "test_error = myNN.squaredError(test_output*classNorm,y_test*classNorm)\n",
        "print(f\"Error on the testing dataset: {test_error}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error on the testing dataset: 2491.0978274797053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL2ldafbjG_J"
      },
      "source": [
        "A sample predicted evaluation of a random poker hand, vs the actual evaluation of the hand provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGy6ZVDumHmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b617bc7d-4a8e-47c9-9788-f0115b07dde0"
      },
      "source": [
        "index = 1001 # checking for random index\n",
        "nInp = list(X_test[index])\n",
        "myNN.feedForward(X_test[index])\n",
        "print(f\"For Input {nInp}\\n => Poker Hand value prediction: {myNN.output_layer.activated_output[0,0]*classNorm}\\n => Actual Value of the hand: {y_test[index,0]*classNorm}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Input [1.0, 0.38461538461538464, 1.0, 0.6923076923076923, 0.75, 1.0, 0.5, 0.5384615384615384, 1.0, 1.0]\n",
            " => Poker Hand value prediction: 0.0\n",
            " => Actual Value of the hand: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUoOtYcJRQ8I"
      },
      "source": [
        "## Error analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIYnVXTaAR2Z"
      },
      "source": [
        "Poker Hands dataset loss function\n",
        "\n",
        "From the graph we can see there is a smooth convergence, that is loss is minimized with every epoch, the network kind of reaches the minimum around the 20th epoch and the error doesnt change much from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQhcCKT6H87G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "47830120-9893-4621-90b8-0517ccb08dcd"
      },
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.title(\"Poker Hands Dataset: Error Convergence\")\n",
        "ax = plt.plot(range(2,30), myNNerror[2:])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABRoAAAHwCAYAAADJtdPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZRlZ1kv/u9T1VOS7nQnIQkhCSSQagaZJxkEwiCDEygXrz/1ChoFFREcrnAFZRAVvD9AFEFGBYeLKCARkKsGw6DMo0BCOiNJCB0y9Jihu6ve+8fZ3V11qnqo7qpzavh81jqrzh7OPu85Z++sXt+8+3mqtRYAAAAAgKMxMuwBAAAAAACLn6ARAAAAADhqgkYAAAAA4KgJGgEAAACAoyZoBAAAAACOmqARAAAAADhqgkYAYOCq6sqqesmwxzFIVXVhVb1t2OMAAID5ImgEAGalqv6yqlr32FNVV1XVn1fVScMe20yq6mVVdekBti2awLP7HHu/9/GqurmqPltVr6iqOxzB8S6tqpfNw1AP5733VNWzjvC1V076HiY/PjjHwzwiVXVsVb2kqr5aVbdU1U1V9Zmqel5VHTvs8QEAzKcVwx4AALAofSLJj6f3b4kHJXlbkjOT/OCwBlRVq1pru4b1/gNyZZKHJ6kkG5I8NMlvJXl2VT2mtfbNIY5tkF6d5I/71t12oJ1nOjeqqpKsaK3tnu2bH+hcq6rjk3wsyZ2S/G6SzyTZmuTBSX41ydVJ/nG27zdIR/O9AACY0QgAHIldrbXvtNauaa19IL3Q58lVdUz1/GZVXV5Vu6rqsqp6wcEOVlVPqKote/erqrVV9fqqurabFfalqvqxSfuf1c1i+6mq+nBV7Uzye0f7oarqJ7vZZ1ur6oaq+lBVbZzhfX+8qj7Yje3y/tl5VXWXqvpIVd1aVVdX1fNmeK+ndp/rlu6zf7aqHnCIIY533/t1rbWLWmvvTPKwJDuS/PmkYz+wqv65qq6vqh1V9bmqevKk7RcmuVuSl06aEXhW99u9tfvNbu0+2x9U1epJrz2jqt7bfT+3dfv8z0nbV3azL6/otn+9qp4zafuVSUaT/MXe9z7EZ57Jju57mPzYMuk9WlX9alX9bVVtTfJXVfWsbiblY6vqS0luT/KEqlpXVW+uqu9W1e1V9fmqeuKkY83mXPv9JPdI8rDW2ptba19urV3RWvv7JI9OcmF3zENeI9WbufmK7jq4qao2V9XrqmpFt/0XuvN0Td/rXlhV36qqkW75nO732lK9WbD/UlX3mbT/gb6XY6rqLd173FxVb6yqP6y+2cFV9RNV9eXut76yql5bVcdN2n5hVb2tqn6nqr7TfZZ3VdXavuP896r6QnecG7vz94RJ259XVRd32zdV1Yv3fhcAwMIhaAQA5sKt6f27YkWSX04viHlVku9J8r+TvKqqzpvphVX1U0nen+QXW2t/XFWV5J+S3C/Jf09y7yRvSvLuqnp838tfneRvun3+PEdvdZJXJnlgku9PMp7kQ1W1qm+/VyV5V5L7Jnl3krdVF0h2439/kpOSnJvkh5P8SHfMvZ/5jkn+Psn/Se87enh6Ye2e2Q64tbY9ve/nMVV1crf6+CR/l+Sx3fv+3yTn1/7Q9MfSmx35miSndY+r05speX2Sn0xyzyQvSPKzSX570lu+Mcn6JE9IL1Q7L8k1k7a/tTv+c7pjvCLJqyf9/g9J73t9waT33vu9tJq727lfmuQ/0/v8e2+PH0nvnPn1buyfT/KOJE9K8tNJ7p/kP5J8sKru0Xe8g55rXbD3U0n+prV2Rf/21rM3DD3ca+R5Sa5L8r3d819J8sxu23uSrEry1L7X/EySv26tTVTVqUk+md5v+qj0QulvJrlw0rlyoO/l1d2x/0f3uq3duCd/5meld+69Jsm9uvd+wgzfz39LcmJ618NPJPmhJC+cdJyfTfLX6c32fGB65+1H0guk050Tv5nkf6V3Tj0/vfPrpQEAFpbWmoeHh4eHh4fHYT+S/GWSf5u0fK8klyX5dLd8dZI/6nvN65JcPmn5yvTCn99ML8B4/KRt56Z3G+z6vmO8I8k/ds/PStKS/M5hjPdlSSbSm/XX/5hI8pKDvPbE7n0e2fe+vz5pn9Ek25M8p1t+QrfPxkn7nJxeGPu2bvkB3T5nzeJ7f1mSSw+w7cnd8R56kNd/JcmLJy1fmuRlh/G+v5ZkU99xZnxdkrO77/Qefet/N8mXJy3vSfKsGV5/cZJfOcR4rkxv1l3/b/lbk/ZpSd7e97pndesfNWndOd26H+jb94tJ3jGbcy3JKf3nxkH2Pdxr5Py+ff45yf+ZtPzuJB+atPzgbgx3n3TOfLrvGJXe9fqCg3wvx3Xf8Xl9r/305HOwG+Mv9u3z6O54J3TLFyb5St8+b0ryqUnL30ryhgN8V8cmuSXJk/vW/0ySLYd7/Xh4eHh4eHgM5uF2AwDgSJxbVTvSC9lWJ7kgyXOqV6PujCQf79v/Y0meX1XHttZu6dY9O71w5pGttS9M2vch6c3UurY3OXCfVUk29R33s4c53quT9M+GTLpbWfeqqvunN0vq/knukF4okyR3SW+m215f3vuktTZeVdcnObVbda8kN7TWLpm0z3eranL9xK+mN8vwa1X1r9043tdau/owP0+/veNs3ec4OcnLkzwuyR3Tm2m6pvscBz9Q1S8k+fn0ArbjutdOvgvmj5O8uaqe0o37Q621vb/3g7uxfL7vt1uR3izGg2qt9c8iPJA/S29m5WQ39i0f6Nz43KTn9+r+9p+vH09vlunhHG+vOsT23k6zu0a+3LfPt9MLc/d6Z3ozVU9prV2fXvj22ba/VudDkjyou1YnOybJWN+6yd/LOeldb5/u2+dT6c3Q3XuO3SXJa6vq/5/8EScdY+8xvzLD53hSd5xT0qvv+i+Z2fd0431v3232o0nWVNXJrbXvHuC1AMCACRoBgCPxmfRu4dyT5Nuta4zRhSiH61PpBWHnVdUXW2t7Q4SR9GY5PmSG1/Q34Nh5mO+1u7U2rfN0Ve2Z9PzY9MKOT6Z3u/DmbtPX0wtdDjaOllmUpOnCyaek9xmfkOTp6d06+4zW2pF0T/6ebgx7b9n9yyR3Tq9RzBXpzaZ8d6Z/jimq6hnphXgvSi/42pbkGenVHtw79r+oqo+kN4vysUn+uare31r76ez/Dh6R3iy0KR/7CD7Xgdw00+/ZZ6ZzY7y1dsCmMUdwvMm+m+Tm7A8v58KhzrN/SXJDkp+sqj9L77bkl03aPpLe/wT4lRmOvXXS8wN9Lwf7zfaO4/lJ/n2G7ZNvpz+a62Xvfs9IcskM2286zOMAAAMgaAQAjsStMwU9rbVtVXVNerdPTg7MHpPkikkztZLkv9Kr33dBkpVV9ewubPx8eh2V17TWvjZvn2C6e6Z3i/OLW2sXJUlVPSKHOVNtkm8kuUNVjbXWNnXHuUOSu6f32ZL0avalN0vus0n+oAvvfjZTv7dDqqp1SX4pyYWttRu61Y9O71bi87t9jkty1ySTv89d6WrgTfLoJF9qrb120vHP6n/P1tp1Sf4ivYYuH07yf6rql5PsnZl650MEpjO99zB8vfv76CQfnrT+0Um+NJsDtV5NxL9NLzj//dZXp7Gr3Xl8a23rLK6RQ73neFX9TXp1FC9Pr3bmuyft8vn0bo2+ZpYB66Xp/UYPT+983uthk957c1Vdnd5t2m+dxbGnaK1d330fT0xy/gy7fD29Ugp3ba19eIbtAMACImgEAObaHyZ5TVVtSu/W2selF4Q9t3/H1trXq+rc9MLGv6iqn0vy0ST/luR9VfVb6d1mfEJ6s+RuO5pQ4xCuSq8u3fOq6jXp3Tr8qsx+Jt4F6d0q+tfV6za9K73GGrv37tAFmI9Pb0badendxnrfJG8/xLFHu0YylV6o9ND0mmocl953vNc3k/xUVX0yvUDvFZke7F2R5JFVdef0Zh/e1L3uvKp6anqh5A+l19hln6p6Q3qh3DfTux37x9K7NX17FzS/I8lbu9/uU93YHpTk5Nbaqye992Or6p/T62B+Q3fsi9Or1feGQ3wPa7vvYbLdrbX+26cPqrV2WVX9fZI3Vq8z9lXpfY/3Tq8hzmy9OL0A8dNV9Tvpzfzdlt6t+L+W5LXpNTw57GvkMLwryW+kd6v8B1trk2f4vSG9Zj0fqKpXpvc7nZHkKend8v6fMx2wtbazqt6c5JVVtTm9mYTPTC+Mn3yb8ouTvL2qbk7ygfTO8XsmeUpr7Tk5fC9P8qbuvf4hvVmMj03y7tbaDVX1B+mF8S29/zasSHKfJA9orb3wQAcFAAZP0AgAzLU3pRcu/XZ6dfSuTvKi1tqMIVpr7eKqekx6AeO70gs0fiS9WomvS3J6eiHYl5P80XwNugs0fjq9EOjnklyUXmfkC2Z5nFZVT0vylvTq8N2QXlfh1ZN225rebLHnpheifie9jsa/d4jDn5VeMDmRXgOaTUnem+RPJs1mTHozI9+c3mzJzel9b8f2Heul3Rj3BoZnd6+5T3qzFVekN+PuZUn+dNLrKr06jWemF1B+Or1gaW8g++z0gq8XpzeLclt6s9Imh4e/kd5ve2WSldk/a/Tu6dXGPJQXZlLX4s7X0wsIZ+vn0/t9/jq9bt3/leSHWmsXz/ZA3WzFh6f3+Z6X5PXpzca7JL1z+/92u87qGjnEe361qr6cXpj5sr5tm7vx/EGS93Wf7ztJPpHeeXQwL0zvvPjb9M63v03vlvx9tU5ba39VVdu7fV+cXimFy7v3ms1neFtV3Zrerf4vSa+5z6fT+03SWvu9qrouvVvAX5NeKYBLuvEAAAtI7f83IQAAwMyq6qNJbm6tPX3YYwEAFiYzGgEAgCmq6j5JHpje7e+r0qsD+dj0brsGAJiRoBEAAOjX0qsb+Sfp1Uy8OMmPttY+MtRRAQALmlunAQAAAICjtmhnNG7dulVCCgAAAABDsH79+upfNzKMgQAAAAAAS4ugEQAAAAA4aoJGWIA2bdo07CHAkuBagrnhWoK54VqCueFagrkxH9eSoBEAAAAAOGqCRgAAAADgqAkaAQAAAICjNrCgsao2VNU/VNXFVXVRVT28qk6sqn+tqk3d3xO6fauq/qSqLq2qr1bVAwc1TgAAAABg9gY5o/H1ST7SWrtHkvsluSjJi5Jc0FobS3JBt5wkT0ky1j2eneRNAxwnAAAAADBLAwkaq2p9kkcneXuStNZ2tda2JHlqknd2u70zydO6509N8q7W8+kkG6rqtEGMFQAAAACYvWqtzf+bVN0/yVuSfCO92YxfSPL8JNe21jZ0+1SSm1trG6rqg0le1Vr7ZLftgiQvbK19fu8xt27dum/gWtsDAAAAwPwaGxvb93z9+vXVv33FgMaxIskDkzyvtfaZqnp99t8mnSRprbWqOqLUc/KHhKVg06ZNzmuYA64lmBuuJZgbriWYG64lmBvzcS0NqkbjNUmuaa19plv+h/SCx817b4nu/l7fbb82yZmTXn9Gtw4AAAAAWIAGEjS21r6T5Oqqunu36vHp3UZ9fpJnduuemeQD3fPzk/xM1336YUm2ttauG8RYAQAAAIDZG9St00nyvCR/U1Wrklye5GfTCzrfU1XnJbkqyY93+344yQ8kuTTJLd2+AAAAAMACNbCgsbX25SQPnmHT42fYtyV57rwPCgAAAACYE4Oq0QgAAAAALGGCRgAAAADgqAkaAQAAAICjJmgEAAAAAI6aoHGBm2gtO3dPDHsYAAAAAHBQA+s6zeH79Obb85aLduaSrXty2dY9+f/OOTavfcSGYQ8LAAAAAA5I0LgA3Xz7RN53xa37li/ZunuIowEAAACAQ3Pr9AK0cf3KKcubtu4Z0kgAAAAA4PAIGhegu6wbzcpJv8zmWyey5XZ1GgEAAABYuASNC9CKkcrdjp96V/ul28xqBAAAAGDhEjQuUGPrpwaNl2xRpxEAAACAhUvQuEBt7A8a1WkEAAAAYAETNC5QY30NYQSNAAAAACxkgsYFqn9Go87TAAAAACxkgsYF6py+oPGKbXuye6INaTQAAAAAcHCCxgXq+FUjOe3Y/T/PntYLGwEAAABgIRI0LmDqNAIAAACwWAgaFzB1GgEAAABYLASNC9hYX9BoRiMAAAAAC5WgcQGbPqNx95BGAgAAAAAHJ2hcwGaa0diaztMAAAAALDyCxgXsTseN5rgVtW95266W62+dGOKIAAAAAGBmgsYFbKQq56jTCAAAAMAiIGhc4HSeBgAAAGAxEDQucNPrNGoIAwAAAMDCI2hc4DauXzll2YxGAAAAABYiQeMCN1PnaQAAAABYaASNC9zdjl+RmrR89Y7x3LJH52kAAAAAFhZB4wK3ZkXlLutGp6y71KxGAAAAABYYQeMioPM0AAAAAAudoHERGOtrCKNOIwAAAAALjaBxETCjEQAAAICFTtC4COg8DQAAAMBCJ2hcBDZumBo0Xrp1dyZaG9JoAAAAAGA6QeMicNLqkZywuvYt3zaeXL1jfIgjAgAAAICpBI2LQFVlY19DGHUaAQAAAFhIBI2LhDqNAAAAACxkgsZFYnrn6d1DGgkAAAAATCdoXCTMaAQAAABgIRM0LhJ336BGIwAAAAALl6Bxkbjz2tGsmvRrXX/rRLbcPjG8AQEAAADAJILGRWLFSOVux/fXaTSrEQAAAICFQdC4iEyv06ghDAAAAAALg6BxEdm4Xp1GAAAAABYmQeMiMrZB52kAAAAAFiZB4yKycb0ajQAAAAAsTILGReScvqDxim17snuiDWk0AAAAALCfoHERWbdyJHc6dv9Ptqf1wkYAAAAAGDZB4yIz1tcQRp1GAAAAABYCQeMio04jAAAAAAuRoHGRGVuv8zQAAAAAC4+gcZHZuKEvaNyye0gjAQAAAID9BI2LTH+Nxk1b96Q1nacBAAAAGC5B4yJzp2NHctyK2re8bXfL5lsnhjgiAAAAABA0LjpVpU4jAAAAAAuOoHERmt55Wp1GAAAAAIZL0LgITZvRuMWMRgAAAACGS9C4CG3cML0hDAAAAAAMk6BxEVKjEQAAAICFRtC4CN113YqM7G88nWt2jmfnbp2nAQAAABgeQeMitGZF5S5rR6esu3SbWY0AAAAADI+gcZGa3nla0AgAAADA8AgaF6mx9VMbwqjTCAAAAMAwCRoXqY0b+mY0bhE0AgAAADA8gsZFanrn6d1DGgkAAAAACBoXrf4ajZdt25PxiTak0QAAAACw3AkaF6mT1ozmxNX7f77bxpOrd44PcUQAAAAALGeCxkVM52kAAAAAFgpB4yI2vU6joBEAAACA4RA0LmLTZjRu0RAGAAAAgOEQNC5iYxvMaAQAAABgYRA0LmIb16+csqxGIwAAAADDImhcxO68djSrJv2C371tIjffPjG8AQEAAACwbAkaF7EVI5W7Hd/feVqdRgAAAAAGT9C4yOk8DQAAAMBCIGhc5KbVadwiaAQAAABg8ASNi5zO0wAAAAAsBAMLGqvqyqr6r6r6clV9vlt3YlX9a1Vt6v6e0K2vqvqTqrq0qr5aVQ8c1DgXm7uv76/RKGgEAAAAYPAGPaPxsa21+7fWHtwtvyjJBa21sSQXdMtJ8pQkY93j2UneNOBxLhrn9AWNV2zfk13jbUijAQAAAGC5Gvat009N8s7u+TuTPG3S+ne1nk8n2VBVpw1jgAvd2pUjOf3Y0X3L460XNgIAAADAIK049C5zpiX5l6pqSd7cWntLklNba9d127+T5NTu+elJrp702mu6dddlBps2bZqfES8Sp69anWtv2R82fuyiqzNyh/Ehjoi5sNzPa5grriWYG64lmBuuJZgbriWYG7O9lsbGxg66fZBB4/e11q6tqlOS/GtVXTx5Y2utdSHkrB3qQy5197txSz67Zee+5R3HnZyxsXVDHBFHa9OmTcv+vIa54FqCueFagrnhWoK54VqCuTEf19LAbp1urV3b/b0+yfuTPDTJ5r23RHd/r+92vzbJmZNefka3jhls7KvTeMmW3UMaCQAAAADL1UCCxqo6rqrW7X2e5IlJvpbk/CTP7HZ7ZpIPdM/PT/IzXffphyXZOukWa/r0B406TwMAAAAwaIO6dfrUJO+vqr3v+bettY9U1eeSvKeqzktyVZIf7/b/cJIfSHJpkluS/OyAxrkoja1fOWV509Y9aa2l+74BAAAAYN4NJGhsrV2e5H4zrL8xyeNnWN+SPHcAQ1sSTjt2JGtXVHbs6ZW43La7ZfOtE7njpG7UAAAAADCfBlajkflTVRnb0Fen0e3TAAAAAAyQoHGJGJtWp1FDGAAAAAAGR9C4RGzsq9N4yRYzGgEAAAAYHEHjEjF9RqOgEQAAAIDBETQuERvXq9EIAAAAwPAIGpeIux6/IiO1f/manePZuXtieAMCAAAAYFkRNC4Rq0crZ60dnbLO7dMAAAAADIqgcQkZ2zC1IYygEQAAAIBBETQuIeo0AgAAADAsgsYlROdpAAAAAIZF0LiETJ/RuHtIIwEAAABguRE0LiH9QeNl2/ZkfKINaTQAAAAALCeCxiXkxDWjOWn1/p/09vHk6p3jQxwRAAAAAMuFoHGJ2bih7/bpLeo0AgAAADD/BI1LTH9DGHUaAQAAABgEQeMSo/M0AAAAAMMgaFxiNq5fOWX5EkEjAAAAAAMgaFxi+jtPm9EIAAAAwCAIGpeYO68dzapJv+oNt03kptt0ngYAAABgfgkal5jRkco5x5vVCAAAAMBgCRqXoLEN/Z2nBY0AAAAAzC9B4xI01tcQxoxGAAAAAOaboHEJ6m8IY0YjAAAAAPNN0LgETe88vXtIIwEAAABguRA0LkHn9AWNV24fz+3jbUijAQAAAGA5EDQuQWtXjuT0Y0f3LY+35Irtbp8GAAAAYP4IGpeoaZ2ntwgaAQAAAJg/gsYlamxanUZBIwAAAADzR9C4RE3vPK0hDAAAAADzR9C4RE3vPG1GIwAAAADzR9C4RG3csHLK8qate9KaztMAAAAAzA9B4xJ1x2NGsm5l7VvevrvlO7dODHFEAAAAACxlgsYlqqqmNYTReRoAAACA+SJoXMKmd57WEAYAAACA+SFoXMI2rp9ap/ESDWEAAAAAmCeCxiVs+oxGQSMAAAAA80PQuIRt3CBoBAAAAGAwBI1L2NnrVmR0f+PpXLNzPDt26zwNAAAAwNwTNC5hq0crZ60bnbLuUrMaAQAAAJgHgsYlbqyvIYzbpwEAAACYD4LGJW5jX0MYnacBAAAAmA+CxiVO52kAAAAABkHQuMRNn9G4e0gjAQAAAGApEzQucf0zGi/btifjE21IowEAAABgqRI0LnEnrhnNHdbs/5lvH0+u3jk+xBEBAAAAsBQJGpeB/lmNl2xRpxEAAACAuSVoXAb66zR+U51GAAAAAOaYoHEZ0HkaAAAAgPkmaFwGNq5fOWVZ0AgAAADAXBM0LgMbN6jRCAAAAMD8EjQuA2ceN5rVo/uXb7x9IjfepvM0AAAAAHNH0LgMjI5U7na8Oo0AAAAAzB9B4zLRX6fxEkEjAAAAAHNI0LhM6DwNAAAAwHwSNC4TG/uCRjMaAQAAAJhLgsZlYtqMxi27hzQSAAAAAJYiQeMycU5f0HjljvHcPt6GNBoAAAAAlhpB4zKxduVIzjhudN/yREsu3+b2aQAAAADmhqBxGem/fVqdRgAAAADmiqBxGdF5GgAAAID5ImhcRqZ3ntYQBgAAAIC5IWhcRsbWr5yybEYjAAAAAHNF0LiMbNzQd+v0lj1pTedpAAAAAI6eoHEZueMxI1m3svYt79jTct0tE0McEQAAAABLhaBxGamqGRrCqNMIAAAAwNETNC4z/UHjJeo0AgAAADAHBI3LzMa+hjCCRgAAAADmgqBxmZl+67SgEQAAAICjJ2hcZmbqPA0AAAAAR0vQuMzcdd2KjO5vPJ1rbxnP9t06TwMAAABwdASNy8yq0crZ66bOarzM7dMAAAAAHCVB4zKk8zQAAAAAc03QuAxtFDQCAAAAMMcEjcvQWH9DmK27hzQSAAAAAJYKQeMy1D+jUedpAAAAAI6WoHEZGlu/csrypdv2ZHyiDWk0AAAAACwFgsZl6ITVIzl5zf6fftdE8q0d40McEQAAAACLnaBxmdJ5GgAAAIC5NNCgsapGq+pLVfXBbvnsqvpMVV1aVX9XVau69au75Uu77WcNcpzLwfTO0xrCAAAAAHDkBj2j8flJLpq0/Ookr2utnZPk5iTndevPS3Jzt/513X7MobENU+s0bjKjEQAAAICjMLCgsarOSPKDSd7WLVeSxyX5h26XdyZ5Wvf8qd1yuu2P7/ZnjkzrPC1oBAAAAOAorDj0LnPmj5P8VpJ13fJJSba01vYmXNckOb17fnqSq5OktbanqrZ2+98w04E3bdo0X2NeslbeVkmO2bd80Y23+x4XGL8HzA3XEswN1xLMDdcSzA3XEsyN2V5LY2NjB90+kKCxqn4oyfWttS9U1blzffxDfUimu+tEy5ovfTu3dc2mt+ypnHjmXXPSmtHhDowkvQvdeQ1Hz7UEc8O1BHPDtQRzw7UEc2M+rqVB3Tr9yCQ/UlVXJnl3erdMvz7JhqraG3aekeTa7vm1Sc5Mkm77+iQ3Dmisy8LoSOVux7t9GgAAAIC5MZCgsbX2v1prZ7TWzkryE0k+2lr7qST/nuS/dbs9M8kHuufnd8vptn+0tdYGMdblZOP6qQ1hLhE0AgAAAHCEBt11ut8Lk/x6VV2aXg3Gt3fr357kpG79ryd50ZDGt6SNbTCjEQAAAIC5MchmMEmS1tqFSS7snl+e5KEz7HNbkmcMdGDLUH/naTMaAQAAADhSw57RyBCN9QeNW3YPaSQAAAAALHaCxmXsnL5mMFftGM9te5TCBAAAAGD2BI3L2HErR3LGcaP7lidacvl2t08DAAAAMHuCxmWuv06jhjAAAAAAHAlB4zKnTiMAAAAAc0HQuMxt3GBGIwAAAABHT9C4zI2tXzll+RJBIwAAAABHQNC4zM1Uo7E1nacBAAAAmB1B4zJ36jEjOX5l7Vveuafl27dMDHFEAAAAACxGgsZlrqqmNYTZtFVDGAAAAABmR9DIDJ2n1WkEAAAAYHYEjWTjhqkNYXSeBgAAAGC2BI1Mn10snioAACAASURBVNEoaAQAAABglgSNzNB5Wo1GAAAAAGZH0EjOXrcio/sbT+fbt0xk+26dpwEAAAA4fIJGsmq0cva6qbMaL3X7NAAAAACzIGgkiTqNAAAAABwdQSNJZqjTuEXQCAAAAMDhEzSSJBnb0D+jUUMYAAAAAA6foJEkM3WeNqMRAAAAgMMnaCRJsnH9yinLl23bkz0TbUijAQAAAGCxOeygsaoeW1Vnd89Pq6p3VtVfVNUd5294DMqG1SM55Zj9p8OuieRbO8aHOCIAAAAAFpPZzGh8Y5K9ydNrkqxMMpHkLXM9KIZjeudpdRoBAAAAODwrDr3LPqe31r5VVSuSPCnJXZLsSvLteRkZA7dx/Yr8x3d27VvetGVPnnzmEAcEAAAAwKIxm6BxW1WdmuTeSb7RWttRVavSm9nIEjDWV6fxEg1hAAAAADhMswka/zTJ55KsSvKCbt0jk1w814NiOHSeBgAAAOBIHXbQ2Fp7dVW9P8l4a+2ybvW1SX5+XkbGwE2v0ShoBAAAAODwzGZGY1prl+x9XlWPTTLRWvvYnI+KoThz7WjWjCa3dS1/brp9IjfeNp6T1owOd2AAAAAALHiH3XW6qj5WVY/snr8wybuT/G1V/fZ8DY7BGqnKOeo0AgAAAHAEDjtoTK8JzKe757+Q5LFJHpbkF+d6UAyPOo0AAAAAHInZ3Do9kqRV1d2SVGvtG0lSVSfMy8gYiml1GrcIGgEAAAA4tNkEjZ9M8oYkpyV5f5J0oeMN8zAuhmT6jMbdQxoJAAAAAIvJbG6dflaSLUm+muRl3bp7JHn93A6JYdJ5GgAAAIAjcdgzGltrNyb57b51H5rzETFU5/QFjVftGM9te1rWrKghjQgAAACAxWA2XadXVtXLq+ryqrqt+/vyqlo1nwNksI5dMZIz147uW55oyeXbzWoEAAAA4OBmc+v0HyV5Qnpdpu/X/X1cklfPw7gYIp2nAQAAAJit2TSDeUaS+3W3UCfJN6vqi0m+kuTX5nxkDM3Y+hW54Nrb9y1fsmV3kmOGNyAAAAAAFrzZzGg8UJE+xfuWmI3rV05ZNqMRAAAAgEOZTdD490n+qaqeVFX3rKonJ/nHJO+Zn6ExLDpPAwAAADBbs7l1+reSvCTJnyW5U5Jrk7w7yep5GBdDNFONxtZaqkxeBQAAAGBmhx00ttZ2Jfnd7pEkqao1SXamF0KyRJxyzEiOX1XZtqslSXbuabl253jOWDubXBoAAACA5WQ2t07PpEWNxiWnqnSeBgAAAGBWjjZoTHphI0vMWF9DGHUaAQAAADiYQ94LW1WPO8jmVXM4FhYQMxoBAAAAmI3DKbr39kNs/9ZcDISFRedpAAAAAGbjkEFja+3sQQyEhWX6jMbdQxoJAAAAAIvBXNRoZAk6+/gVWTGpzc91t0xk266J4Q0IAAAAgAVN0MiMVo5Uzj5+6qzGS90+DQAAAMABCBo5IHUaAQAAADhcgkYOSJ1GAAAAAA6XoJEDMqMRAAAAgMMlaOSANq5fOWV5k6ARAAAAgAMQNHJA/TMaL9u2J3sm2pBGAwAAAMBCJmjkgDasHskpx+w/RXZPJFdtHx/iiAAAAABYqASNHNT0Oo0awgAAAAAwnaCRg5reeVqdRgAAAACmEzRyUGN9DWF0ngYAAABgJoJGDsqMRgAAAAAOh6CRg+qv0fjNLbvTms7TAAAAAEwlaOSgzlw7mmNGa9/yll0tN94+McQRAQAAALAQCRo5qJGqnNPfeXqL26cBAAAAmErQyCGp0wgAAADAoQgaOaT+Oo06TwMAAADQT9DIIU2f0bh7SCMBAAAAYKESNHJIYxtWTlk2oxEAAACAfoJGDulux4+mJi1ftX08t+1pQxsPAAAAAAuPoJFDOnbFSM5cO7pvuSW5bJtZjQAAAADsJ2jksOg8DQAAAMDBCBo5LNM7T2sIAwAAAMB+gkYOy8b1UxvCmNEIAAAAwGSCRg7L2Ib+GY2CRgAAAAD2EzRyWGaq0TjRdJ4GAAAAoEfQyGE5ec1I1q+qfcu37Gn59s7xIY4IAAAAgIVE0MhhqSqdpwEAAAA4IEEjh22sryGMOo0AAAAA7CVo5LCZ0QgAAADAgQgaOWxj63WeBgAAAGBmAwkaq2pNVX22qr5SVV+vqpd368+uqs9U1aVV9XdVtapbv7pbvrTbftYgxsnBbdzQP6Nx95BGAgAAAMBCM6gZjbcneVxr7X5J7p/kyVX1sCSvTvK61to5SW5Ocl63/3lJbu7Wv67bjyE7a92KrNjfeDrX3TKRbbsmhjcgAAAAABaMgQSNrWdHt7iye7Qkj0vyD936dyZ5Wvf8qd1yuu2Pr6pJERfDsHKkctfjp85qvNTt0wAAAAAkWXHoXeZGVY0m+UKSc5L8WZLLkmxpre1Nqq5Jcnr3/PQkVydJa21PVW1NclKSG2Y69qZNm+Zx5Ex2p9FVuWTSafPxb16TdVvGhziipct5DXPDtQRzw7UEc8O1BHPDtQRzY7bX0tjY2EG3DyxobK2NJ7l/VW1I8v4k95irYx/qQzJ3HrB1ay68ace+5W3HnJSxsfVDHNHStGnTJuc1zAHXEswN1xLMDdcSzA3XEsyN+biWBt51urW2Jcm/J3l4kg1VtTfsPCPJtd3za5OcmSTd9vVJbhzwUJnBtM7TW9w6DQAAAMDguk6f3M1kTFUdk+T7k1yUXuD437rdnpnkA93z87vldNs/2lprgxgrB7dxw8opy5vUaAQAAAAgg7t1+rQk7+zqNI4keU9r7YNV9Y0k766qVyb5UpK3d/u/PclfVdWlSW5K8hMDGieHcE5fM5jLt+/J7omWlSN69QAAAAAsZwMJGltrX03ygBnWX57koTOsvy3JMwYwNGZpw+qRnHrMSDbfOpEk2T2RXLV9T85Zv/IQrwQAAABgKRt4jUYWv2l1Gt0+DQAAALDsCRqZtY3r1WkEAAAAYCpBI7NmRiMAAAAA/QSNzNrGDVODxk1bBI0AAAAAy52gkVmbPqNxd1prQxoNAAAAAAuBoJFZO+O40RwzWvuWt+xqueG2iSGOCAAAAIBhEzQyayNVOUedRgAAAAAmETRyRDb2BY06TwMAAAAsb4JGjshMdRoBAAAAWL4EjRyRaTMadZ4GAAAAWNYEjRyRsQ0rpyyr0QgAAACwvAkaOSLnHL8iNWn5WzvGc+ueNrTxAAAAADBcgkaOyDErKndeO7pvuSW5bJtZjQAAAADLlaCRIza987SGMAAAAADLlaCRIza2ob/ztBmNAAAAAMuVoJEjtnH91IYwH7jy1tymTiMAAADAsiRo5Ig9+ORVU5a/cfOevORzW4c0GgAAAACGSdDIEbv3iSvzo2cdM2Xd2y7emfdfccuQRgQAAADAsAgaOSp//MgNOWvd6JR1v/ofW3K5DtQAAAAAy4qgkaOyftVI/vLcE7Nq0pm0fXfLs/79JvUaAQAAAJYRQSNH7f53WJVXPmT9lHVfvWl3fke9RgAAAIBlQ9DInPiFex6XH7nLminr3nrxznzgyluHNCIAAAAABknQyJyoqvzp952Qu6ydWq/xeZ+8OVeo1wgAAACw5AkamTPrV43kLx97YlZOOqu27W551oU35fZx9RoBAAAAljJBI3PqATPUa/zKjeo1AgAAACx1gkbm3LPveVx+uK9e41suUq8RAAAAYCkTNDLnqip/+sgTcucZ6jVeuV29RgAAAIClSNDIvNiweiR/ee70eo0/q14jAAAAwJIkaGTePPDkVXnFg6fWa/zSDbvzu+o1AgAAACw5gkbm1S/e67j84J2n1mt880U7c756jQAAAABLiqCReVVV+bPvm16v8Vf+Q71GAAAAgKVE0Mi827B6JH/RX69xV8vPXXhTdqnXCAAAALAkCBoZiAedvCov76vX+MUbdud3P69eIwAAAMBSIGhkYH7pXsflB/rqNf75N3bmg1ep1wgAAACw2AkaGZiqyhu/74Sc2Vev8bmfVK8RAAAAYLETNDJQe+s1rqj967aq1wgAAACw6AkaGbgHn7wqL3vw8VPWffGG3XnZF9RrBAAAAFisBI0MxXO/Z22efObUeo1v/PrOfEi9RgAAAIBFSdDIUFRV3vSoE3LGcVPrNf7yJ2/OVeo1AgAAACw6gkaG5oQD1Gs872PqNQIAAAAsNoJGhuohp6zKS/vqNX7+u7vz8i9sG9KIAAAAADgSgkaG7ldmqNf4Z1/fkQ9/S71GAAAAgMVC0MjQHbBe4yduzrd2qNcIAAAAsBgIGlkQTlg9knece8KUeo1bdrX83IXqNQIAAAAsBoJGFoyHnrI6L33Q9HqNr1CvEQAAAGDBEzSyoDz33mvzpDNWT1n3hq/vyD+r1wgAAACwoAkaWVBGunqNpx87tV7jL33i5lytXiMAAADAgiVoZME5cc1o3nHuCRmdoV7j7gn1GgEAAAAWIkEjC9L3nro6v9tXr/Fz392d31OvEQAAAGBBEjSyYD3v3mvzxL56jX/ytR35yNXqNQIAAAAsNIJGFqy99RrvdOzU0/SXPnFzrlGvEQAAAGBBETSyoJ20ZjRvP/fEKfUab7695byP3axeIwAAAMACImhkwXv4qavzOw+cWq/xM9fvyivVawQAAABYMASNLAq/ep+1+f7Tp9ZrfP3XduT/Xn3bkEYEAAAAwGSCRhaFkaq86dHqNQIAAAAsVIJGFo07zFCv8abbJ9RrBAAAAFgABI0sKg8/dXVePEO9xt//onqNAAAAAMMkaGTRecF91ubxffUa//i/duRfr1GvEQAAAGBYBI0sOiNVefOjT8hpffUan/Pxm3PtzvEhjQoAAABgeRM0sijdYc1o3vaYEzPSV6/x5z92U/ao1wgAAAAwcIJGFq1H3nF1XvyAqfUaP7V5V/7gS+o1AgAAAAyaoJFF7dfuuzaPu9PUeo2v/eqO/Jt6jQAAAAADJWhkUdtbr/GOx0yv1/ht9RoBAAAABkbQyKJ38jGjedu5U+s13nj7RM5TrxEAAABgYASNLAnfd8fV+e0Z6jX+oXqNAAAAAAMhaGTJ+PX7rs1jZ6jXeMG16jUCAAAAzDdBI0vGSFXe0levsSV59sfUawQAAACYb4JGlpSTjxnNWx8zvV7jz6vXCAAAADCvBI0sOY86bXVedP91U9b95+ZdedWXtw9pRAAAAABLn6CRJek37rsu5/bVa3zNV7bno+o1AgAAAMwLQSNL0uhIr17jqf31Gj9+c761Y8/wBgYAAACwRAkaWbJOmaFe4w23TeTh778+f/Tlbbllz8TwBgcAAACwxAgaWdIefdrqvLCvXuPOPS1/8KXtech7r897LrslE02TGAAAAICjJWhkyfvN+67LU85cM239tbeM59kfvzlP/NB389nrbx/CyAAAAACWDkEjS97oSOWvHndiXvPw9Tlx9fRT/vPf3Z0nfuiGnHfhTeo3AgAAABwhQSPLwoqRynn3WJsvPv3U/Mr3rM3KGc78915xax76vs35vS9szfbd6jcCAAAAzIagkWVlw+qRvPKh6/OZHz01P3Tn6bdT3zaevOarO/Lg927OX12yM+MT6jcCAAAAHA5BI8vSXY9fkb9+/En5pyffIfc5ceW07Ztvncjz/mNLzv2n7+YT16nfCAAAAHAoAwkaq+rMqvr3qvpGVX29qp7frT+xqv61qjZ1f0/o1ldV/UlVXVpVX62qBw5inCw/jzptdS784ZPzp4/ckFOPmX45/NdNu/PDH7khP3XBjbl8m/qNAAAAAAcyqBmNe5L8RmvtXkkeluS5VXWvJC9KckFrbSzJBd1ykjwlyVj3eHaSNw1onCxDoyOV/7HxuHz+6afmN++7LmtGp+/zoW/dlu99/+a85LNbs+V29RsBAAAA+g0kaGytXdda+2L3fHuSi5KcnuSpSd7Z7fbOJE/rnj81ybtaz6eTbKiq0wYxVpavdStH8pIHHZ/P/tipefrZx0zbvnsiecPXd+RB792ct120I3vUbwQAAADYp1obbFhSVWcl+XiSeyf5VmttQ7e+ktzcWttQVR9M8qrW2ie7bRckeWFr7fN7j7N169Z9A9+0adPgPgDLxle3jeR1V6zM17bPMMUxydnHTuQFZ+/KI04wwxEAAABY+sbGxvY9X79+ffVvXzHIwVTV2iTvTfKC1tq2XrbY01prVXVEqefkDwlzZSzJjz6w5b2X35qXf2Fbrtk5PmX7FbeM5PlfX5MnnL46r3zo+txjw/SmMkdq06ZNzmuYA64lmBuuJZgbriWYG64lmBvzcS0NrOt0Va1ML2T8m9ba+7rVm/feEt39vb5bf22SMye9/IxuHQzUSFWecbdj89kfOyW//YB1OXbFtLA+/3bt7XnkP16f//mpLbnxtvEZjgIAAACw9A2q63QleXuSi1prr5206fwkz+yePzPJByat/5mu+/TDkmxtrV03iLHCTI5dMZLfuv/x+cLTT81PnnNs+uPG8Za89eKdecB7N+cNX9ueXePqNwIAAADLy6BmND4yyf9I8riq+nL3+IEkr0ry/VW1KckTuuUk+XCSy5NcmuStSX55QOOEgzrt2NG88VEn5N9/+OQ84tRV07Zv29Xyks9ty8PevzkfvOrWDLoGKgAAAMCwDKRGY9fUZfo9pz2Pn2H/luS58zooOAr3v8OqfOgpd8j5V92Wl35+a67cPvWW6cu3j+enP3pTHnXHVfn9h67PfU+aHkoCAAAALCUDq9EIS01V5alnHZPP/OipecWDj8/xK6dn6Z/4zq485vzv5nmfvDmbb1G/EQAAAFi6BI1wlFaPVn71Puvyhaefmp+7+3EZ6csbW5K/2nRLHvTezXnNV7bn1j1upwYAAACWHkEjzJGTjxnNax+xIZ986il57J1WT9u+Y0/L731xWx7yvs157+W3qN8IAAAALCmCRphj9zphZd73xJPyd084KWPrp5dBvWbneM772M158odvyBe+u2sIIwQAAACYe4JGmAdVlSeduSb/+bRT8urvXZ8TVk+v3/iZ63fl8R/8bp79sZtyzY49QxglAAAAwNwRNMI8WjlSec691uaLT79jfulex2XFDL3X33P5rXnI+67P739xW3bsnhj8IAEAAADmgKARBuCE1SP5w+/dkE/96Cl58plrpm2/dbzlf39lex783s35m007M6F8IwAAALDICBphgMbWr8y7n3BS/vFJJ+VeJ0yv3/idWyfy3E9uyTO/vCbvuHhnrrtlfAijBAAAAJg9QSMMwbl3WpNP/Mgpef0jNuTkNdMvw4t3juTXP7Ul9/y77+Tx/3R9XvOV7f+vvbuPkeO+7zv++c7MPt0D73gUyfDpSMkiZNkRoziWYwOOn+K4Dt0isuEEMRDAdls3aFrURYAmRVrAbowmRtsYDZonIK0RJ239AMe144SOY6eOI1etVEuVqCfLtGzy+EyK5PGe9m53Z379Y2Z3Zmd3ybvbO+4e+X4Bh535ze9p725viA9/M6MXrtV5UjUAAAAAABhanUuqANwSvmd6/32jevfdFX3i+Lx+77kF1brcovGJl+t64uW6PvbknO4e93V0uqKj02W9fldRvtflpo8AAAAAAAADwIpGYMC2FT199LUTevw9u/XwocoN6/5gPtTvPregd33lZR3+zAX940eu6cunqlrkITIAAAAAAGDAWNEIDIlD44H+6K1TenG2rk89eUaPLY7qiZfrPetfXYn06e8t6dPfW1LZl968t6x3TZf1zgNl7ar4t3DmAAAAAAAABI3A0LlvsqAPHmjoNw7v0vmlUH85s6xjM1V98/xK10urJWk5lL56ellfPb0sk/S6XUUdnS7r6HRZhycKt3T+AAAAAADgzkTQCAyxPSO+PvjKUX3wlaOar0f66zMrOjZT1VfPLOt6rfuDYZykxy7V9Nilmj7y7Tkdngh09EAcOj60qyjPuK8jAAAAAADYeASNwBYxXvD08N0VPXx3RfXI6dELNR2bqerY6WWdXgh7tjtxvaHfvr6g3352QTvLnt6ZhI5v2VtWJSB0BAAAAAAAG4OgEdiCCp7pzXtLevPekj7+407PXmvEoePMsp6+0vu+jpeXI/3JiSX9yYkljQSmt+0t6eh0WX/nQFk7ytzXEQAAAAAArB9BI7DFmZkemCrogamCfvXBbTqz0NBXTi/r2MyyHjm/okb3K6y11HD685ll/fnMsjyTXp/c1/Fd0xXdvY0/DQAAAAAAYG1IE4DbzP6xQB+6f0wfun9MsyuRvn42Dh2/fmZZc/XuqWPkpEcv1vToxZr+9f+d0/2TQfIwmYp+9K4C93UEAAAAAAA3RdAI3MYmS57ee8+I3nvPiGqh07curOhY8hTrc0s9HmEt6YXZhl6YXdBvHV/QnhFPP32goqPTZf3EnpJKPqEjAAAAAADoRNAI3CGKvult+8p6276y/v3rJ/T0lbr+YmZZfzFT1fPXGj3bnV+K9MkXF/XJFxc1XjD95L74YTLv2F/WZMm7he8AAAAAAAAMM4JG4A5kZnrwrqIevKuof/WabTo532itdPzfF2sKe9zXcb7u9MWTVX3xZFWBSW/YXdTrdhV1ZEdRR6YKOjTuy7jMGgAAAACAOxJBIwAdGg/0S68e0y+9ekzXViJ99XQcOv712RUt9niaTMNJj1yo6ZELtVbZtoLph6cKOrKjoCNTBR3ZUdR9k4EKHuEjAAAAAAC3O4JGAG22lzz9/L0j+vl7R7TccPrb8ys6NlPVV04v62K1930dJWmu7loPlWkq+dL9k9nwsaBXby9otMBl1wAAAAAA3E4IGgH0VA5M7zhQ1jsOlPUJ5/TE5bqOzVR1bGZZL17vfV/HrJVQeupKXU9dqbfKTNLhiaAtfDwyVdBU2d+kdwIAAAAAADYbQSOAVfHM9NCuoh7aVdRHXjuh78819Nilmo5fqen41bqeuVrXXK3HzR1znKTvXm/ou9cb+vz3q63y/aO+HphqX/24f5T7PgIAAAAAsBUQNAJYl3u2BbpnW6D33TsiSXLO6dRCqKev1PXMlbqOX63p+JW6LtzkcuusM4uhziyG+srp5VbZ9pLpyFQxDSB3FHR4WyCf+z4CAAAAADBUCBoBbAgz06HxQIfGA/3MoUqr/FI11PErdR2/WtfxK3U9c7Wml+bCVfd7bcXpm+dX9M3zK62yim969VSgI1PF1urHV20vqBwQPgIAAAAAMCgEjQA21a6Kr7fv9/X2/eVW2Xw90rNJ8NgMIL8zW1d9lYsfq6HTty/X9e3L6X0ffZPumwj0wI74addHpgp6YKqgyRIPnQEAAAAA4FYgaARwy40XPL1hd0lv2F1qla2ETt+ZTcPHZ67E931cbKzuvo+hk56fbej52YY++1J638eDY76O7CjoFdsCTY8Fmh7zdXDc14HRgBWQAAAAAABsIIJGAEOh5Jt+ZEdRP7Kj2CqLnNP35xptKx+PX63r5eXV3/fx1EKoUwvdL9XeXfF0cCzQ9LgfB5BJEDk9Fmj/mK+STxAJAAAAAMBqETQCGFqeme6dKOjeiYLec09c5pzT+aWo9bCZZvg40yNMvJGL1UgXqzU9frnzmEnaM+LFqyDH/XQ15Figg+O+9o36KvBAGgAAAAAAWggaAWwpZqa9o772jlb0zgPpQ2dmV6Jk1WOtden1i9cbilZ35XUHJ+ncUqRzSzX9n0udxz2T9o74yQpIX9PjgQ6OpYHkvlFfAUEkAAAAAOAOQtAI4LYwWfL0pj0lvWlPet/HasPp+Wt1vTBb1+mFUKfmG5pZCDWzEOrcUrjuEFKSIiedWQx1ZjHUoxc7j/sm7RtNLskeTy/JPpgEk3tGfPkEkQAAAACA2whBI4DbViUw/djOon5sZ7HjWC10OrcU6tR8qFMLzQCy0Qokzy9F6iOHVOjUCjW/daHWcbzgSftH0xWQzUDyh0Z87ap42lX2tL3kyYwwEgAAAACwNRA0ArgjFX3TofFAh8YDSaWO4yuh09nF7CrIhk4thJqZj7cvVFf/QJpu6pH0g/lQP5jvfW/JwKSdFU87y752VzztrMQh5M6Kr11lr7W9uxKHkh6hJAAAAABggAgaAaCLkm+6Z1uge7Z1/zNZbTidWYxDyFNJ+NgKJOdDXV7Dk7F7aTjp/FKk80s378s3aWc5E0aWPe1Ktne1yuLXqZLHZdsAAAAAgA1H0AgA61AJTIcnCjo8Ueh6fKkRJZdhpyFk6xLt+VBXVvoPIrNCJ12oRqtaaemZdFcziCx72lnJbrevntxBKAkAAAAAWCWCRgDYBCOBp/smPd032T2InK/HQeTMQkMz86FOLYQ6vdDQpWqkS9V4ReR8vZ+7RPYWOSXjrC6U3FHKhJEVT7vKvqbKniaLniaLpslSvL295Gmy5GlbwQgnAQAAAOAORNAIAAMwXvD0qu2eXrW9exApxZdnN0PHi0vx66VqqMvVSJeWQ12qRvF2NdTcJoaSl5cjXV6O9Py1xqrbbStaEkR6SRCZBpKt144y00SRFZQAAAAAsFURNALAkKoEpoPjgQ6O37zucsPp0nLYCh7jUDLSxWq+LNT12uaEkllzNae5WqgZ9X7YTS/bCqaJnmGkp8mS5QLLuGyi6CkgpAQAAACAgSFoBIDbQDkwTY8Fmh67ed2V0PVcGXl5uT2cnL0FoWTeXN1prh7q9DpCyvFCHDg2V0j6taJ2n7+q8YKnscA0WjCNFTyNFUzjyfZoYMl+XD5W8FT2JeMp3gAAAACwJgSNAHCHKfmmA2OBDqwilKyFLr50uhqmKySXI11biTS7Emm21nx1mq1Fur4Sbdpl3KsxX3ear4c6s9gMKQPpanXN/fgmjRZM40EzfDSNFtLt8UxAmQ0uR4P2OnE7U8U3gksAAAAAtz2CRgBAT0XftG/U175Rf9VtGpHTXC0JH7uEkR1lzf1apLkBrKDsJnTp5d8bIRtcjhbaA8qxgmks8FQJTJXANBLEweRIsn+zskpg8ggxAQAAAAwBgkYAwIYKPNNU6WfEowAADwZJREFU2ddUee1tw8hprp4PKCPNruRCyi5lczWn4YgpO210cJlX9uN7eo74ucCyW0Dp5453KSu32nitQLPE5eQAAAAAboKgEQAwNHzPtL1k2l7y1tw2ck5zuVWTL546p/G7dmuh7rTYcFqoR5qvOy3U4+24zGm+HiVlTouNSCubkwdumuVQWg6drq3jvparZZJGAlPJN5V9Ja+We+1VHgeVrf2gs7zodamfqRcYQScAAAAw7AgaAQC3Bc8sfiJ1yZOSJ3XvWwx1+PDomvuqhXEw2QwgF+vZkDK67YPLbpykxUb8vgfBM3UGlj2CzlISXJZ8qeA19+NbAZQ8U8E3lZL9opce62iTtCv5lpQlxzxT0ReXrAMAAAA5BI0AAOQUk5BpPSsru6lHaQC5mAkm5zMB5XLDaSl0qjaclhrxazXMbDfLM3WWk+N3gshJSw2npYakIblIPjB1DSOLXhpStsLMTL1suFlItgPPVPDitkHyWmhu+6aCxXWKSdhZSOpnX9N+4u1CW1+sCAUAAMDmI2gEAGCTFfq4JPxmnHNaDqVqI2oFkd3CybayMHe8rSzqGnTWog2f+pbXcFKj4bQkaVjCzxsJTB0BZZAJJQt+csxMBb89vFxZLGry7FUFzTAzCT4DTwosbuc3+8zut9VLw8/As577vrXPLztGx5hcUg8AADBUCBoBANjCzEyVQKoEvqY2cZxGFAePtTAONlfCeEVl56vS/UaP8o7XuL+VyGml0b3+HbJwc1M1nNQInaqhtPZgNJCuVDdhVv0LTAq8NKRshpaBZ/JMre3A4vCzGXr6ZvKT0LJVlmnrmzr689te4z6CTB9tx1v9t4/f1tbiS/D97JxM8r10u1m3+V5afbS1TefrEb4CAIABImgEAAA3FXimcc+kwmDGb0Rx6Lhyw6AzDSdXQqd6pOQ13q9F8f03V6L0WC1yqoVKXjPtkv1mm2y9Zl8YDnGAGv/sY6TS2YCyGUD6mUDTy4WYfibAzIaknuX7SNv6mQC0s561Qk8/Nw/rUs83yfM6y61LPT8JkDvaZ+bm5ebQ3PcydbvNPT1ubduEuAAArB5BIwAAGHrxJbOm0QEFnXnOxYFkLRNIdg0uI6VBZyaobAadjShuW4+cGkl/9SQIzb/Worh+uh0fq0VxEFsLnequuS3VXdInK0LvOKGTwo6Vq/wSbARTZ1iZDzqz+9lQ1DOpUS+r/NzFXNApeYrDXVO3IDQNaL0ux/NjeUoCWaVhrddlrl6zbu64ZftXrr51n2Ncz2Rt++3hb9w2N2+117HmWFKuXvv3x+uYXzK+lJtDfo6ExQCw2QgaAQAA1sgsfjBL0beBrfJci2YwuubwMjl2+ux57dz9Q3Eg6qRGcrzupDCp00jGyO5n6zWS/jvrxWOGURqONpJxmgFsdj9bLyQ7wy3mFP/Oxrlt/hdwNb+QnpKnWmGA0sAyDTKzQap1KU9D1iRQVXvI2Qx2W6+5kNMy/VkmsG32mQ1x03lYe/ukj2yYqnzQ26PfVpts4JvrV7kwufV+1VluSWjb+b7T/lv18v21fU+adaxL3fT7Zrm5nZ31dO7cStfvZ3acbLkyP5u0Xrf26XjZ35fsz6Rb++a46ThpefZ9dPu+EIDjdkLQCAAAcJtrC0bX4UQt1OFXjGzwrPoXOacwF0iGLrmcurmdhJRhEnKG2RA0CUrjNml42WgFqvGx9jbNPjLHM8eyfbS/Zo8326XHI9c+/ygzr2bdMGpvFzW3M3MCcHNOan2W0pJeNTGcytKzLw96EhuuI5zMh59KQ0q1hZXWpU17wCl1CThbbaxtv/3Veswp2186vtrGzrTvMbZ1qdM+lvWYV7Zu9/bdgufs966zP+sxv/TnoEyZ12Meb91b0mt2Fvv9ddiyCBoBAACwJTVX6xSay07QCl9bwWouxGwPKdMQNswFmo1kO9tftk6U6bsZtkbd+nJpIBpGTpGS9l3qtfqL2sdJx8jNIeqcj1M6hyiz7TJ9tOaudB5Rrl0UKZlrHOJGiucGAJup7e9M29+cm/0B4g/UMBkJJggaAQAAAGx9afgqpRcLYiO4JPTMh5Jh1Fwhl4aSzbC0GXxGivd/cPKU9k8fbAWoza8w17dTGrC2HW99ZULbzHHnOgPaODh1rbA0f9y5ZH6ZcLU5jmu2z46vNDR22TmpyxxdGtJm553OtXsdlykLc2O7/PzV3re7wbwJiwHcCnf6lfAEjQAAAABwE60nZmv9Qa4/6nR4agvc2PU25pzLhJPdQ1KpPRTNB6UuH3DmwlKnTGDaCkTbA9Rm+/x88v26TH2Xm0OU6SMbpKZ9ua79ts2/R7/NwLbZh8vPW2m4HUlSrq/O+ml5dvxm0O1cZ32XqZ9/z0tLSypXKq26ao2dfK87vm/JdrPfzH7++5vWS8dVbv7534Pmz7fj/bX2Xdt7zo6N288dnjMSNAIAAAAA7gzNe6i133HhTo8Ftp4TJ07o8OHpQU9jw3QLVdP9LsFoElA2g9PONtn99uNSNvDsDGXbw9J0Xul42f5dZ7CaG7ttv9m+td+lfUd47Dr7bb2HzmNqG797yN257VrP+Ip61bvBPJT5XjonPbjjzv4PJYJGAAAAAACAAWkG4EpWTeeO3vL5AP3wBj0BAAAAAAAAAFsfQSMAAAAAAACAvhE0AgAAAAAAAOgbQSMAAAAAAACAvhE0AgAAAAAAAOgbQSMAAAAAAACAvhE0AgAAAAAAAOgbQSMAAAAAAACAvhE0AgAAAAAAAOgbQSMAAAAAAACAvhE0AgAAAAAAAOgbQSMAAAAAAACAvhE0AgAAAAAAAOgbQSMAAAAAAACAvhE0AgAAAAAAAOibOecGPYd1uX79+tacOAAAAAAAALDFTUxMWL6MFY0AAAAAAAAA+kbQCAAAAAAAAKBvW/bSaQAAAAAAAADDgxWNAAAAAAAAAPpG0AgAAAAAAACgbwSNwJAxs5Nm9oyZPWVm3x70fICtwsw+aWaXzOzZTNmUmX3NzE4kr9sHOUdgK+jxWfqomZ1Nzk1PmdnRQc4RGHZmdsDMvmFmz5vZc2b24aSc8xKwBjf4LHFeAtbAzMpm9riZPZ18lv5NUn63mT1mZt8zs8+aWbHvsbhHIzBczOykpNc6514e9FyArcTM3iRpQdIfO+d+OCn7d5KuOuc+bmb/UtJ259yvDnKewLDr8Vn6qKQF59x/GOTcgK3CzPZI2uOce9LMxiU9IelhSR8Q5yVg1W7wWfo5cV4CVs3MTNKoc27BzAqSviXpw5J+WdIXnHOfMbM/kPS0c+73+xmLFY0AgNuCc+5vJV3NFf+MpE8l259S/A9TADfQ47MEYA2cc+edc08m2/OSXpC0T5yXgDW5wWcJwBq42EKyW0i+nKS3Sfp8Ur4h5yWCRmD4OEl/ZWZPmNk/GvRkgC1ut3PufLJ9QdLuQU4G2OL+qZkdTy6t5nJPYJXM7JCkH5X0mDgvAeuW+yxJnJeANTEz38yeknRJ0tckvSRp1jnXSKqc0QYE+QSNwPB5o3PuNZJ+WtI/SS5hA9AnF98rhPuFAOvz+5JeIelBSecl/dZgpwNsDWY2JulPJf1z59xc9hjnJWD1unyWOC8Ba+ScC51zD0raL+l1kl65GeMQNAJDxjl3Nnm9JOl/KP4DAGB9Lib39mne4+fSgOcDbEnOuYvJP04jSX8ozk3ATSX3wPpTSf/NOfeFpJjzErBG3T5LnJeA9XPOzUr6hqQ3SJo0syA5tF/S2X77J2gEhoiZjSY3OZaZjUp6h6Rnb9wKwA38maT3J9vvl/SlAc4F2LKawUji3eLcBNxQctP9/yLpBefcJzKHOC8Ba9Drs8R5CVgbM9tpZpPJdkXSTym+5+k3JL03qbYh5yWeOg0METO7R/EqRkkKJP1359y/HeCUgC3DzD4t6S2S7pJ0UdJHJH1R0uckTUs6JennnHM85AK4gR6fpbcovjzNSTop6Rcz95kDkGNmb5T0iKRnJEVJ8a8pvrcc5yVglW7wWXqfOC8Bq2ZmRxQ/7MVXvOjwc865X08yiM9ImpL0/yT9gnNupa+xCBoBAAAAAAAA9ItLpwEAAAAAAAD0jaARAAAAAAAAQN8IGgEAAAAAAAD0jaARAAAAAAAAQN8IGgEAAAAAAAD0jaARAAAAQ8fMnJndO+h5AAAAYPUIGgEAAHBTZnbSzKpmtpD5+p1BzwsAAADDIxj0BAAAALBl/D3n3NcHPQkAAAAMJ1Y0AgAAYN3M7ANm9r/M7HfM7LqZfcfMfjJzfK+Z/ZmZXTWz75nZhzLHfDP7NTN7yczmzewJMzuQ6f7tZnbCzGbN7HfNzJJ295rZN5PxXjazz97CtwwAAIAeWNEIAACAfv24pM9LukvSeyR9wczuds5dlfQZSc9K2ivplZK+ZmYvOef+p6RflvQ+SUclfVfSEUlLmX7/rqSHJG2T9ISkL0v6S0kfk/RXkt4qqSjptZv9BgEAAHBz5pwb9BwAAAAw5MzspOIgsZEp/heS6pJ+Q9I+l/zD0swel/SfJP2NpJOSJp1z88mx35S0xzn3ATN7UdKvOOe+1GU8J+knnHPfSvY/J+lJ59zHzeyPJS1L+nXn3JlNeLsAAABYBy6dBgAAwGo97JybzHz9YVJ+1rX/7/UpxSsY90q62gwZM8f2JdsHJL10g/EuZLaXJI0l278iySQ9bmbPmdnfX+f7AQAAwAYiaAQAAEC/9jXvn5iYlnQu+Zoys/HcsbPJ9mlJr1jrYM65C865Dznn9kr6RUm/Z2b3rm/qAAAA2CgEjQAAAOjXLkn/zMwKZvazku6XdMw5d1rSo5J+08zKZnZE0j+Q9F+Tdv9Z0sfM7LDFjpjZjpsNZmY/a2b7k91rkpykaKPfFAAAANaGh8EAAABgtb5sZmFm/2uSviTpMUmHJb0s6aKk9zrnriR13ifpDxSvbrwm6SPOua8nxz4hqaT4wS53SfqOpHevYh4PSfqPZjaRjPdh59z3+3ljAAAA6B8PgwEAAMC6mdkHJP1D59wbBz0XAAAADBaXTgMAAAAAAADoG0EjAAAAAAAAgL5x6TQAAAAAAACAvrGiEQAAAAAAAEDfCBoBAAAAAAAA9I2gEQAAAAAAAEDfCBoBAAAAAAAA9I2gEQAAAAAAAEDf/j+FzlfE6i6wtgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1ziM4NRa8-"
      },
      "source": [
        "**Observations:** As the depth or width of the neural network was increased I had to lower the learning rate or else the network used to either just jump around a value trying to converge or get stuck on a random error.\n",
        "\n",
        "**Conclusion:** Neural Netowrks are pretty robust for many applications, while the functionality and the math behind the network is all linear but a combination of these linear nodes (ReLU activation) are able to adjust for the non-linearity required for many diffrent applications. The XOR function is an example where a NN is able to create a non-linear boundary where as any linear estimation will fail (a simple perceptron network will fail)\n",
        "\n",
        "**Future work:** Implementaion of Binary Cross entropy or Cross entropy for the error/loss functions for classification will be an improvement. \n"
      ]
    }
  ]
}